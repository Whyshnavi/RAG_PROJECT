# RAG_PROJECT
This project implements a Retrieval-Augmented Generation (RAG) pipeline using LangChain and Google Generative AI embeddings.
The system retrieves relevant context from external sources and augments the input prompt for improved responses from LLMs

#Features

Load documents from the web using LangChain WebBaseLoader
Split text into chunks with RecursiveCharacterTextSplitter
Store and query embeddings using Chroma Vector Database
Use Google Generative AI embeddings for semantic search
Combine retrieval + LLM with a custom prompting pipeline
End-to-end RAG workflow in a Jupyter Notebook

#Tech Stack

Python 3.12
LangChain
Chroma
Google Generative AI Embeddings
BeautifulSoup (bs4)
OS & Warnings (utility)

# Project Structure
RAG_project.ipynb   # Main notebook containing the RAG pipeline
README.md           # Project documentation
requirements.txt    # Python dependencies (optional)

# Installation

Clone this repository:

git clone https://github.com/WHYSHNAVI/RAG_PROJECT.git
cd rag-project


Create a virtual environment and install dependencies:

pip install -r requirements.txt


Add your Google Generative AI API Key:

export GOOGLE_API_KEY="your_api_key"

# Usage

Run the Jupyter Notebook:

jupyter notebook RAG_project.ipynb


Inside the notebook:

  Load your dataset/web content.
  Split text into chunks.
  Store embeddings in Chroma.
  Query with RAG pipeline.

# Example Workflow

Input query: "Explain LangChain RAG in simple terms"

   Retriever fetches context from knowledge base. 
   LLM generates an answer using query + context.

# Future Improvements

Add support for multiple vector DBs (Pinecone, Weaviate).

Create a Streamlit/Gradio app for interactive demo.

Deploy RAG pipeline as an API.
